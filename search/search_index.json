{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"install/","text":"Prerequisites Provide access to Kubernetes cluster. You can also run it locally within a minikube . Make sure to provision enough memory (8G+) and CPU (8CP+) resources: All MacOS minikube start --memory = 8196 --cpus 8 minikube start --driver = hyperkit --memory = 8196 --cpus 8 Install Operator This oneliner installs the Streaming Runtime Operator in your Kubernetes environment: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-operator/install.yaml' -n streaming-runtime It installs the streaming-runtime operator along with the custom resource definitions (CRDs) (such as ClusterStream , Stream and Processor ) and required roles and binding configurations.","title":"Install"},{"location":"install/#prerequisites","text":"Provide access to Kubernetes cluster. You can also run it locally within a minikube . Make sure to provision enough memory (8G+) and CPU (8CP+) resources: All MacOS minikube start --memory = 8196 --cpus 8 minikube start --driver = hyperkit --memory = 8196 --cpus 8","title":"Prerequisites"},{"location":"install/#install-operator","text":"This oneliner installs the Streaming Runtime Operator in your Kubernetes environment: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-operator/install.yaml' -n streaming-runtime It installs the streaming-runtime operator along with the custom resource definitions (CRDs) (such as ClusterStream , Stream and Processor ) and required roles and binding configurations.","title":"Install Operator"},{"location":"streaming-runtime-build/","text":"Build & Run Streaming Runtime Operator build instructions to build the operator, create a container image and upload it to container registry. CRDs Every time the CRDs under the ./crds folder are modified make sure to runt the regnerate the models and installation. Generate CRDs Java api and models ./scripts/generate-streaming-runtime-crd.sh Generated code is under the ./streaming-runtime/src/generated/java/com/vmware/tanzu/streaming folder Build operator installation yaml ./scripts/build-streaming-runtime-operator-installer.sh producing the install.yaml . The ./scripts/all.sh combines above two steps. Build the operator code and image ./mvnw clean install -Pnative -DskipTests spring-boot:build-image docker push ghcr.io/vmware-tanzu/streaming-runtimes/streaming-runtime:0.0.3-SNAPSHOT (For no-native build remove the -Pnative ). User Defined Functions - follow the User Defined Function about information how implement and build your own UDF and how to use it from within a Processor resource.","title":"Build"},{"location":"streaming-runtime-build/#build-run","text":"","title":"Build &amp; Run"},{"location":"streaming-runtime-build/#streaming-runtime-operator","text":"build instructions to build the operator, create a container image and upload it to container registry.","title":"Streaming Runtime Operator"},{"location":"streaming-runtime-build/#crds","text":"Every time the CRDs under the ./crds folder are modified make sure to runt the regnerate the models and installation. Generate CRDs Java api and models ./scripts/generate-streaming-runtime-crd.sh Generated code is under the ./streaming-runtime/src/generated/java/com/vmware/tanzu/streaming folder Build operator installation yaml ./scripts/build-streaming-runtime-operator-installer.sh producing the install.yaml . The ./scripts/all.sh combines above two steps.","title":"CRDs"},{"location":"streaming-runtime-build/#build-the-operator-code-and-image","text":"./mvnw clean install -Pnative -DskipTests spring-boot:build-image docker push ghcr.io/vmware-tanzu/streaming-runtimes/streaming-runtime:0.0.3-SNAPSHOT (For no-native build remove the -Pnative ).","title":"Build the operator code and image"},{"location":"streaming-runtime-build/#user-defined-functions-","text":"follow the User Defined Function about information how implement and build your own UDF and how to use it from within a Processor resource.","title":"User Defined Functions -"},{"location":"streaming-runtime-usage/","text":"Streaming pipeline deployment With this you are set to deploy your streaming pipelines. For example, you can deploy the anomaly-detection sample: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/streaming-pipeline.yaml' -n streaming-runtime and the data-generator that simulates the input authorization streams for this use case: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/data-generator.yaml' -n default You can find more examples in Streaming Runtime Samples . In general the streaming data pipeline implementation would like this: Your streaming data pipeline is implementing by defining Stream and Processor custom resources along with custom user defined functions (UDF) implemented in you language of choice. The Stream input and output resources are used to model the access to your messaging infrastructure (aka Kafka, Pulsar or RabbitMQ), the messaging streams (like topics or exchanges) as well as the schema of the data that flows through those Streams. The Processor CRD defines how the input should be processed in order to produce the output streams. You can (optionally) provide Streaming (SQL) Queries that can aggregate, join , re-arrange or just cleanse the input streams before later are passed to the multibinder and your UDF implementations. If you decide not to use streaming query then the entire architecture might look like this: E.g. you only define the input and output Stream s and the Processor to wire them with your custom UDF implementation. Check the user-defined-functions to learn how to build UDFs. NOTE: There is an undergoing work for implementing a very basic Time Windowed Aggregation that would allow the developers to calculate aggregates in near-real time directly in their UDFs. This feature would provide an alternative (simple) way to build analytics in addition to elaborate Streaming SQL features explained above. Finally, you can chain multiple Streams and Processors (e.g. the output Stream of one Processor is used an input of another) to build streaming data pipeline.","title":"Usage"},{"location":"streaming-runtime-usage/#streaming-pipeline-deployment","text":"With this you are set to deploy your streaming pipelines. For example, you can deploy the anomaly-detection sample: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/streaming-pipeline.yaml' -n streaming-runtime and the data-generator that simulates the input authorization streams for this use case: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/data-generator.yaml' -n default You can find more examples in Streaming Runtime Samples . In general the streaming data pipeline implementation would like this: Your streaming data pipeline is implementing by defining Stream and Processor custom resources along with custom user defined functions (UDF) implemented in you language of choice. The Stream input and output resources are used to model the access to your messaging infrastructure (aka Kafka, Pulsar or RabbitMQ), the messaging streams (like topics or exchanges) as well as the schema of the data that flows through those Streams. The Processor CRD defines how the input should be processed in order to produce the output streams. You can (optionally) provide Streaming (SQL) Queries that can aggregate, join , re-arrange or just cleanse the input streams before later are passed to the multibinder and your UDF implementations. If you decide not to use streaming query then the entire architecture might look like this: E.g. you only define the input and output Stream s and the Processor to wire them with your custom UDF implementation. Check the user-defined-functions to learn how to build UDFs. NOTE: There is an undergoing work for implementing a very basic Time Windowed Aggregation that would allow the developers to calculate aggregates in near-real time directly in their UDFs. This feature would provide an alternative (simple) way to build analytics in addition to elaborate Streaming SQL features explained above. Finally, you can chain multiple Streams and Processors (e.g. the output Stream of one Processor is used an input of another) to build streaming data pipeline.","title":"Streaming pipeline deployment"},{"location":"samples/instructions/","text":"Use-case layout All use-cases are organized in folders named of after the use-case, each containing two files: streaming-runtime-samples/ <use-case-folder>/ streaming-pipeline.yaml data-generator.yaml The streaming-pipeline.yaml is a Kubernetes manifest that uses the Streaming-Runtime Custom Resources, such as ClusterStream , Stream and Processor , to define decoratively the data processing pipeline. It defines the input and output streams as well as the processing queries and the UDF functions to be applied. The data-generator.yaml is a Kubernetes deployment manifest that continuously generates realistic test date for this use case. Depends on the Use Case one or more threads can be deployed to pump concurrently messages to the scenarios' input streams. Run Follow the Streaming Runtime installation instructions to install the operator. Next from within the streaming-runtime-samples directory, deploy the use-case streaming pipeline: kubectl apply -f '<use-case-folder>/streaming-pipeline.yaml' -n streaming-runtime and the data generator to provide test data for this use case: kubectl apply -f '<use-case-folder>/data-generator.yaml' -n streaming-runtime Note Substitute the <use-case-folder> placeholder with the folder name of the use-case of choice. Explore the Results As the use-case input and output streams are backed by messaging systems such as Apache Kafka or RabbitMQ we can explore the content of those messages that fly through the pipeline. Kafka Topics Use the kubectl get all to find the Kafka broker pod name and then kubectl exec -it pod/<your-kafka-pod> -- /bin/bash ` to SSH to kafka broker container. From within the kafka-broker container use the bin utils to list the topics or check their content: /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092 Then to list the topic content: /opt/kafka/bin/kafka-console-consumer.sh --topic <topic-name> --from-beginning --bootstrap-server localhost:9092 To delete a topic: /opt/kafka/bin/kafka-topics.sh --delete --topic <topic-name> --bootstrap-server localhost:9092 Rabbit Queues To access the Rabbit management UI first forward the 15672 port: kubectl port-forward svc/rabbitmq 15672 :15672 Then open http://localhost:15672/#/exchanges and find the exchange name related to your use-case. Open the Queues tab and create new queue called myTempQueue (use the default configuration). Go back to the Exchang tab, select the use-case exchange and bind it to the new myTempQueue queue, with # as a Routing key ! From the Queue tab select the myTempQueue queue and click the Get Messages button.","title":"Instructions"},{"location":"samples/instructions/#use-case-layout","text":"All use-cases are organized in folders named of after the use-case, each containing two files: streaming-runtime-samples/ <use-case-folder>/ streaming-pipeline.yaml data-generator.yaml The streaming-pipeline.yaml is a Kubernetes manifest that uses the Streaming-Runtime Custom Resources, such as ClusterStream , Stream and Processor , to define decoratively the data processing pipeline. It defines the input and output streams as well as the processing queries and the UDF functions to be applied. The data-generator.yaml is a Kubernetes deployment manifest that continuously generates realistic test date for this use case. Depends on the Use Case one or more threads can be deployed to pump concurrently messages to the scenarios' input streams.","title":"Use-case layout"},{"location":"samples/instructions/#run","text":"Follow the Streaming Runtime installation instructions to install the operator. Next from within the streaming-runtime-samples directory, deploy the use-case streaming pipeline: kubectl apply -f '<use-case-folder>/streaming-pipeline.yaml' -n streaming-runtime and the data generator to provide test data for this use case: kubectl apply -f '<use-case-folder>/data-generator.yaml' -n streaming-runtime Note Substitute the <use-case-folder> placeholder with the folder name of the use-case of choice.","title":"Run"},{"location":"samples/instructions/#explore-the-results","text":"As the use-case input and output streams are backed by messaging systems such as Apache Kafka or RabbitMQ we can explore the content of those messages that fly through the pipeline.","title":"Explore the Results"},{"location":"samples/instructions/#kafka-topics","text":"Use the kubectl get all to find the Kafka broker pod name and then kubectl exec -it pod/<your-kafka-pod> -- /bin/bash ` to SSH to kafka broker container. From within the kafka-broker container use the bin utils to list the topics or check their content: /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092 Then to list the topic content: /opt/kafka/bin/kafka-console-consumer.sh --topic <topic-name> --from-beginning --bootstrap-server localhost:9092 To delete a topic: /opt/kafka/bin/kafka-topics.sh --delete --topic <topic-name> --bootstrap-server localhost:9092","title":"Kafka Topics"},{"location":"samples/instructions/#rabbit-queues","text":"To access the Rabbit management UI first forward the 15672 port: kubectl port-forward svc/rabbitmq 15672 :15672 Then open http://localhost:15672/#/exchanges and find the exchange name related to your use-case. Open the Queues tab and create new queue called myTempQueue (use the default configuration). Go back to the Exchang tab, select the use-case exchange and bind it to the new myTempQueue queue, with # as a Routing key ! From the Queue tab select the myTempQueue queue and click the Get Messages button.","title":"Rabbit Queues"},{"location":"samples/overview/","text":"Following samples demonstrate how to use the Streaming Runtime to implement various streaming and event-driven use case scenarios: Anomaly Detection - detect, in real time, suspicious credit card transactions, and extract them for further processing. Clickstream Analysis - for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. IoT Monitoring analysis - real-time analysis of IoT monitoring log. Streaming Music Service - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time. ... more to come","title":"Overview"},{"location":"samples/anomaly-detection/anomaly-detection/","text":"Credit Card Anomaly Detection Imagine a stream of credit card authorization attempts, representing, for example, people swiping their chip cards into a reader or typing their number into a website. Such stream may look something like this: { \"card_number\" : \"1212-1221-1121-1234\" , \"card_type\" : \"discover\" , \"card_expiry\" : \"2013-9-12\" , \"name\" : \"Mr. Chester Stracke\" }, { \"card_number\" : \"1234-2121-1221-1211\" , \"card_type\" : \"dankort\" , \"card_expiry\" : \"2012-11-12\" , \"name\" : \"Preston Abbott\" } ... Then we would like to identify the suspicious transactions, in real time, and extract them for further investigations. We can express such validation using the following streaming SQL query: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INSERT INTO [[ STREAM : possible - fraud - stream ]] SELECT window_start , window_end , card_number , COUNT ( * ) AS authorization_attempts FROM TABLE ( TUMBLE ( TABLE [[ STREAM : card - authorizations - stream ]], DESCRIPTOR ( event_time ), INTERVAL '5' SECONDS ) ) GROUP BY window_start , window_end , card_number HAVING COUNT ( * ) > 5 Here we group the incoming authorization attempts by the card numbers ( lines: 12-13 ) and look only at those authorizations that have the same card number occurring suspiciously often ( 14-15 ). Then we put the suspicious card numbers into a new stream ( 1 ). But it would make no sense to count throughout the entire history of the authorization attempts! We are only interested in frequent authorization attempts that happen in short intervals of time. For this we split the incoming stream into a series of fixed-sized, non-overlapping and contiguous time intervals called Tumbling Windows ( 6-10 ). Here we aggregate the stream in intervals of 5 seconds assuming that 5 authorization attempts in 5 seconds would be hard for a person to do. Swiping the card or submitting the form five times within five seconds is a little weird. If we see that happening it is flagged as a possible fraud and inserted to the possible-fraud-stream ( 1 ). Note that the input stream does not provide a time field for the time when the authorization attempt was performed. Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional event_time field to the authorization attempts data schema. Next we can register a custom function (UDF) to the new, possible-fraud stream to investigate the suspicious transactions further or for example to send warning emails and downstream messages. The UDF function can be implemented in any programming language as long as they adhere to the Streaming-Runtime gRPC protocol. Following diagram illustrates the implementation flow and involved resources: Quick start Follow the Streaming Runtime Install instructions to instal the operator. Install the anomaly detection streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/streaming-pipeline.yaml' -n streaming-runtime Install the authorization attempts random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/data-generator.yaml' -n default Follow the explore Kafka and explore Rabbit to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = authorization-attempts-data-generator Implementation details One possible way of implementing the above scenario with the help of the Streaming Runtime is to define three Streams and one Processor custom resources and use the Processor's built-in query capabilities. (Note: for the purpose of the demo we will skip the explicit CusterStream definitions and instead will enable auth-provisioning for those). Given that the input authorization attempts stream uses an Avro data format like this: { \"name\" : \"AuthorizationAttempts\" , \"namespace\" : \"com.tanzu.streaming.runtime.anomaly.detection\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"card_number\" , \"type\" : \"string\" }, { \"name\" : \"card_type\" , \"type\" : \"string\" }, { \"name\" : \"card_expiry\" , \"type\" : \"string\" }, { \"name\" : \"name\" , \"type\" : \"string\" } ] } We can represent it with the following custom Stream resource: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : card-authorizations-stream spec : protocol : \"kafka\" storage : clusterStream : \"card-authorizations-cluster-stream\" streamMode : [ \"read\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : AuthorizationAttempts fields : - name : card_number type : string - name : card_type type : string - name : card_expiry type : string - name : name type : string - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The event_time field is auto-provisioned and assigned with Kafka message's timestamp. In addition, 3 seconds watermark is configured for the event_time field to tolerate out of order or late coming messages! Then the new possible-fraud-stream populated from the fraud detection processor (using JSON format): apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : possible-fraud-stream spec : protocol : \"kafka\" storage : clusterStream : \"possible-fraud-stream-cluster-stream\" streamMode : [ \"read\" , \"write\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : PossibleFraud fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : card_number type : string - name : authorization_attempts type : long options : ddl.key.fields : card_number ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset The streaming Processor can be defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : possible-fraud-processor spec : # the input and output streams references must use the [[STREAM:<stream-name>]] syntax. query : - \"INSERT INTO [[STREAM:possible-fraud-stream]] SELECT window_start, window_end, card_number, COUNT(*) AS authorization_attempts FROM TABLE(TUMBLE(TABLE [[STREAM:card-authorizations-stream]], DESCRIPTOR(event_time), INTERVAL '5' SECONDS)) GROUP BY window_start, window_end, card_number HAVING COUNT(*) > 5\" # UDF configuration inputs : # input streams for the UDF function - name : \"possible-fraud-stream\" # This is the output of the TWA query above. outputs : # output streams for the UDF function - name : \"udf-output-possible-fraud-stream\" template : spec : containers : - name : possible-fraud-analysis-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the udf-output-possible-fraud-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-possible-fraud-stream spec : keys : [ \"card_id\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"udf-output-possible-fraud-cluster-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"Anomaly Detection"},{"location":"samples/anomaly-detection/anomaly-detection/#credit-card-anomaly-detection","text":"Imagine a stream of credit card authorization attempts, representing, for example, people swiping their chip cards into a reader or typing their number into a website. Such stream may look something like this: { \"card_number\" : \"1212-1221-1121-1234\" , \"card_type\" : \"discover\" , \"card_expiry\" : \"2013-9-12\" , \"name\" : \"Mr. Chester Stracke\" }, { \"card_number\" : \"1234-2121-1221-1211\" , \"card_type\" : \"dankort\" , \"card_expiry\" : \"2012-11-12\" , \"name\" : \"Preston Abbott\" } ... Then we would like to identify the suspicious transactions, in real time, and extract them for further investigations. We can express such validation using the following streaming SQL query: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INSERT INTO [[ STREAM : possible - fraud - stream ]] SELECT window_start , window_end , card_number , COUNT ( * ) AS authorization_attempts FROM TABLE ( TUMBLE ( TABLE [[ STREAM : card - authorizations - stream ]], DESCRIPTOR ( event_time ), INTERVAL '5' SECONDS ) ) GROUP BY window_start , window_end , card_number HAVING COUNT ( * ) > 5 Here we group the incoming authorization attempts by the card numbers ( lines: 12-13 ) and look only at those authorizations that have the same card number occurring suspiciously often ( 14-15 ). Then we put the suspicious card numbers into a new stream ( 1 ). But it would make no sense to count throughout the entire history of the authorization attempts! We are only interested in frequent authorization attempts that happen in short intervals of time. For this we split the incoming stream into a series of fixed-sized, non-overlapping and contiguous time intervals called Tumbling Windows ( 6-10 ). Here we aggregate the stream in intervals of 5 seconds assuming that 5 authorization attempts in 5 seconds would be hard for a person to do. Swiping the card or submitting the form five times within five seconds is a little weird. If we see that happening it is flagged as a possible fraud and inserted to the possible-fraud-stream ( 1 ). Note that the input stream does not provide a time field for the time when the authorization attempt was performed. Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional event_time field to the authorization attempts data schema. Next we can register a custom function (UDF) to the new, possible-fraud stream to investigate the suspicious transactions further or for example to send warning emails and downstream messages. The UDF function can be implemented in any programming language as long as they adhere to the Streaming-Runtime gRPC protocol. Following diagram illustrates the implementation flow and involved resources:","title":"Credit Card Anomaly Detection"},{"location":"samples/anomaly-detection/anomaly-detection/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Install the anomaly detection streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/streaming-pipeline.yaml' -n streaming-runtime Install the authorization attempts random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/data-generator.yaml' -n default Follow the explore Kafka and explore Rabbit to see what data is generated and how it is processed though the pipeline. To delete the data pipeline and the data generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = authorization-attempts-data-generator","title":"Quick start"},{"location":"samples/anomaly-detection/anomaly-detection/#implementation-details","text":"One possible way of implementing the above scenario with the help of the Streaming Runtime is to define three Streams and one Processor custom resources and use the Processor's built-in query capabilities. (Note: for the purpose of the demo we will skip the explicit CusterStream definitions and instead will enable auth-provisioning for those). Given that the input authorization attempts stream uses an Avro data format like this: { \"name\" : \"AuthorizationAttempts\" , \"namespace\" : \"com.tanzu.streaming.runtime.anomaly.detection\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"card_number\" , \"type\" : \"string\" }, { \"name\" : \"card_type\" , \"type\" : \"string\" }, { \"name\" : \"card_expiry\" , \"type\" : \"string\" }, { \"name\" : \"name\" , \"type\" : \"string\" } ] } We can represent it with the following custom Stream resource: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : card-authorizations-stream spec : protocol : \"kafka\" storage : clusterStream : \"card-authorizations-cluster-stream\" streamMode : [ \"read\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : AuthorizationAttempts fields : - name : card_number type : string - name : card_type type : string - name : card_expiry type : string - name : name type : string - name : event_time type : long_timestamp-millis metadata : from : timestamp readonly : true watermark : \"`event_time` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The event_time field is auto-provisioned and assigned with Kafka message's timestamp. In addition, 3 seconds watermark is configured for the event_time field to tolerate out of order or late coming messages! Then the new possible-fraud-stream populated from the fraud detection processor (using JSON format): apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : possible-fraud-stream spec : protocol : \"kafka\" storage : clusterStream : \"possible-fraud-stream-cluster-stream\" streamMode : [ \"read\" , \"write\" ] keys : [ \"card_number\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.anomaly.detection name : PossibleFraud fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : card_number type : string - name : authorization_attempts type : long options : ddl.key.fields : card_number ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset The streaming Processor can be defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : possible-fraud-processor spec : # the input and output streams references must use the [[STREAM:<stream-name>]] syntax. query : - \"INSERT INTO [[STREAM:possible-fraud-stream]] SELECT window_start, window_end, card_number, COUNT(*) AS authorization_attempts FROM TABLE(TUMBLE(TABLE [[STREAM:card-authorizations-stream]], DESCRIPTOR(event_time), INTERVAL '5' SECONDS)) GROUP BY window_start, window_end, card_number HAVING COUNT(*) > 5\" # UDF configuration inputs : # input streams for the UDF function - name : \"possible-fraud-stream\" # This is the output of the TWA query above. outputs : # output streams for the UDF function - name : \"udf-output-possible-fraud-stream\" template : spec : containers : - name : possible-fraud-analysis-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the udf-output-possible-fraud-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-possible-fraud-stream spec : keys : [ \"card_id\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"udf-output-possible-fraud-cluster-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"Implementation details"},{"location":"samples/clickstream/clickstream/","text":"Clickstream Analysis The Streaming ETL is a common place for many people to begin first when they start exploring the streaming data processing. With the streaming ETL we get some kind of data events coming into the streaming pipeline, and we want intelligent analysis to go out the other end. Let's just say clickstream data is coming in, and we want to know who are our high status customers who are currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. Clickstream data is the pathway that a user takes through their online journey. For a single website it generally shows how the user progressed from search to purchase. The clickstream links together the actions a single user has taken within a single session. This means identifying where a search, click or purchase was performed within a single session. Let say that our streaming ETL, receives two input streams of data: the Users stream - containing detailed information about the registered users: { \"user_id\" : \"407-41-3862\" , \"name\" : \"Olympia Koss\" , \"level\" : \"SILVER\" } { \"user_id\" : \"066-68-4140\" , \"name\" : \"Dr. Leah Daniel\" , \"level\" : \"GOLD\" } { \"user_id\" : \"722-61-1415\" , \"name\" : \"Steven Moore\" , \"level\" : \"GOLD\" } { \"user_id\" : \"053-26-9971\" , \"name\" : \"Karine Boyle\" , \"level\" : \"SILVER\" } { \"user_id\" : \"795-35-5070\" , \"name\" : \"Dr. Akiko Hoppe\" , \"level\" : \"PLATINUM\" } { \"user_id\" : \"164-85-7495\" , \"name\" : \"Kassie Homenick\" , \"level\" : \"SILVER\" } ... and the Clicks - the stream of click events: { \"user_id\" : \"170-65-1094\" , \"page\" : 5535 , \"action\" : \"selection\" , \"device\" : \"computer\" , \"agent\" : \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991\" } { \"user_id\" : \"804-31-3496\" , \"page\" : 30883 , \"action\" : \"checkout\" , \"device\" : \"tablet\" , \"agent\" : \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\" } { \"user_id\" : \"011-54-8948\" , \"page\" : 18877 , \"action\" : \"products\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\" } { \"user_id\" : \"854-27-6546\" , \"page\" : 64282 , \"action\" : \"selection\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0\" } { \"user_id\" : \"719-81-5311\" , \"page\" : 54344 , \"action\" : \"product_detail\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/20100101 Firefox/7.0.1\" } { \"user_id\" : \"263-22-2309\" , \"page\" : 28687 , \"action\" : \"products\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (iPhone; CPU iPhone OS 12_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0 Mobile/15E148 Safari/604.1\" } Note the JSON format is optional. For real scenarios Apache Avro is most likely to be used Then we can define a streaming query that continuously analyses the two input streams and produces a new stream containing only enriched information for the Platinum level users. 1 . INSERT INTO VipActions 2 . SELECT 3 . Users . user_id , Users . name , Clicks . page , Clicks . action , Clicks . event_time 4 . FROM 5 . Clicks 6 . INNER JOIN 7 . Users ON Clicks . user_id = Users . user_id 8 . WHERE 9 . Users . level = 'PLATINUM' The first line ( 1 ) creates a new VIP Actions stream and a new Kafka topic. The query is going to run and never stop until we kill it. The new stream is created as an ordinary SQL select ( 2 ) from a stream called Clicks ( 5 ) joined to a stream called Users ( 6-7 ). Since the Clicks and Users streams have both a user_id we can join them to get a new stream that's the clickstream data enriched with the user data. We are only interested in platinum-level users. This the user-level information is not available in the Clicks stream, but as we were able to join to the Users stream now we can filter based on the Users data ( 8-9 ). Following diagram illustrates the implementation flow and involved resources: Quick start Follow the Streaming Runtime Install instructions to instal the operator. Install the Clickstream pipeline: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/streaming-pipeline.yaml' -n streaming-runtime Install the click-stream random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/data-generator.yaml' -n default Follow the explore Kafka and explore Rabbit to see what data is generated and how it is processed though the pipeline. Delete the Top-k songs pipeline and the demo song generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = clickstream-data-generator","title":"Clickstream Analysis"},{"location":"samples/clickstream/clickstream/#clickstream-analysis","text":"The Streaming ETL is a common place for many people to begin first when they start exploring the streaming data processing. With the streaming ETL we get some kind of data events coming into the streaming pipeline, and we want intelligent analysis to go out the other end. Let's just say clickstream data is coming in, and we want to know who are our high status customers who are currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day. Clickstream data is the pathway that a user takes through their online journey. For a single website it generally shows how the user progressed from search to purchase. The clickstream links together the actions a single user has taken within a single session. This means identifying where a search, click or purchase was performed within a single session. Let say that our streaming ETL, receives two input streams of data: the Users stream - containing detailed information about the registered users: { \"user_id\" : \"407-41-3862\" , \"name\" : \"Olympia Koss\" , \"level\" : \"SILVER\" } { \"user_id\" : \"066-68-4140\" , \"name\" : \"Dr. Leah Daniel\" , \"level\" : \"GOLD\" } { \"user_id\" : \"722-61-1415\" , \"name\" : \"Steven Moore\" , \"level\" : \"GOLD\" } { \"user_id\" : \"053-26-9971\" , \"name\" : \"Karine Boyle\" , \"level\" : \"SILVER\" } { \"user_id\" : \"795-35-5070\" , \"name\" : \"Dr. Akiko Hoppe\" , \"level\" : \"PLATINUM\" } { \"user_id\" : \"164-85-7495\" , \"name\" : \"Kassie Homenick\" , \"level\" : \"SILVER\" } ... and the Clicks - the stream of click events: { \"user_id\" : \"170-65-1094\" , \"page\" : 5535 , \"action\" : \"selection\" , \"device\" : \"computer\" , \"agent\" : \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991\" } { \"user_id\" : \"804-31-3496\" , \"page\" : 30883 , \"action\" : \"checkout\" , \"device\" : \"tablet\" , \"agent\" : \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\" } { \"user_id\" : \"011-54-8948\" , \"page\" : 18877 , \"action\" : \"products\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\" } { \"user_id\" : \"854-27-6546\" , \"page\" : 64282 , \"action\" : \"selection\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0\" } { \"user_id\" : \"719-81-5311\" , \"page\" : 54344 , \"action\" : \"product_detail\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/20100101 Firefox/7.0.1\" } { \"user_id\" : \"263-22-2309\" , \"page\" : 28687 , \"action\" : \"products\" , \"device\" : \"mobile\" , \"agent\" : \"Mozilla/5.0 (iPhone; CPU iPhone OS 12_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0 Mobile/15E148 Safari/604.1\" } Note the JSON format is optional. For real scenarios Apache Avro is most likely to be used Then we can define a streaming query that continuously analyses the two input streams and produces a new stream containing only enriched information for the Platinum level users. 1 . INSERT INTO VipActions 2 . SELECT 3 . Users . user_id , Users . name , Clicks . page , Clicks . action , Clicks . event_time 4 . FROM 5 . Clicks 6 . INNER JOIN 7 . Users ON Clicks . user_id = Users . user_id 8 . WHERE 9 . Users . level = 'PLATINUM' The first line ( 1 ) creates a new VIP Actions stream and a new Kafka topic. The query is going to run and never stop until we kill it. The new stream is created as an ordinary SQL select ( 2 ) from a stream called Clicks ( 5 ) joined to a stream called Users ( 6-7 ). Since the Clicks and Users streams have both a user_id we can join them to get a new stream that's the clickstream data enriched with the user data. We are only interested in platinum-level users. This the user-level information is not available in the Clicks stream, but as we were able to join to the Users stream now we can filter based on the Users data ( 8-9 ). Following diagram illustrates the implementation flow and involved resources:","title":"Clickstream Analysis"},{"location":"samples/clickstream/clickstream/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Install the Clickstream pipeline: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/streaming-pipeline.yaml' -n streaming-runtime Install the click-stream random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/data-generator.yaml' -n default Follow the explore Kafka and explore Rabbit to see what data is generated and how it is processed though the pipeline. Delete the Top-k songs pipeline and the demo song generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = clickstream-data-generator","title":"Quick start"},{"location":"samples/iot-monitoring/iot-monitoring/","text":"Real Time IoT Log Monitoring Imagine an IoT network, such as network of sensors, emitting monitoring events into a central service, into a topic iot-monitoring-stream . Such a monitoring stream may look something like this: { \"error_code\" : \"C009_OUT_OF_RANGE\" , \"ts\" : 1645020042399 , \"type\" : \"ERROR\" , \"application\" : \"Hatity\" , \"version\" : \"1.16.4 \" , \"description\" : \"Chuck Norris can binary search unsorted data.\" } { \"error_code\" : \"C014_UNKNOWN\" , \"ts\" : 1645020042400 , \"type\" : \"DEBUG\" , \"application\" : \"Mat Lam Tam\" , \"version\" : \"5.0.9 \" , \"description\" : \"Chuck Norris doesn't bug hunt, as that signifies a probability of failure. He goes bug killing.\" } { \"error_code\" : \"C005_FAILED_PRECONDITION\" , \"ts\" : 1645020042400 , \"type\" : \"ERROR\" , \"application\" : \"Regrant\" , \"version\" : \"2.4.2 \" , \"description\" : \"There is no Esc key on Chuck Norris' keyboard, because no one escapes Chuck Norris.\" } { \"error_code\" : \"C012_UNAVAILABLE\" , \"ts\" : 1645020042401 , \"type\" : \"INFO\" , \"application\" : \"Zontrax\" , \"version\" : \"6.17.2 \" , \"description\" : \"Chuck Norris does not use exceptions when programming. He has not been able to identify any of his code that is not exceptional.\" } { \"error_code\" : \"C006_INTERNAL\" , \"ts\" : 1645020042402 , \"type\" : \"WARN\" , \"application\" : \"Vagram\" , \"version\" : \"5.9.3 \" , \"description\" : \"Quantum cryptography does not work on Chuck Norris. When something is being observed by Chuck it stays in the same state until he's finished.\" } { \"error_code\" : \"C011_RESOURCE_EXHAUSTED\" , \"ts\" : 1645020042402 , \"type\" : \"INFO\" , \"application\" : \"Cardguard\" , \"version\" : \"7.12.19 \" , \"description\" : \"All browsers support the hex definitions #chuck and #norris for the colors black and blue.\" } { \"error_code\" : \"C012_UNAVAILABLE\" , \"ts\" : 1645020042402 , \"type\" : \"ERROR\" , \"application\" : \"Zaam-Dox\" , \"version\" : \"3.12.18 \" , \"description\" : \"Chuck Norris can use GOTO as much as he wants to. Telling him otherwise is considered harmful.\" } { \"error_code\" : \"C013_UNIMPLEMENTED\" , \"ts\" : 1645020042403 , \"type\" : \"INFO\" , \"application\" : \"Aerified\" , \"version\" : \"3.16.11-SNAPSHOT\" , \"description\" : \"Chuck Norris's first program was kill -9.\" } ... The ts field contains the time (e.g. timestamp) when the monitoring event was emitted. From the monitoring stream we will do some filtering ( 12 ), because we only want the monitoring events the represent error events. Then we will group by error_code ( 12-13 ) so that the newly computed, output stream is going to have just the error events and a count of how often each error event occur. The last we will perform time windowing aggregation ( 6-10 ) so that this output would be how many times each error event has occurred in the most recent 1 minute . We can express such analysis with a streaming SQL query like this: 1 . INSERT INTO [[ STREAM : error - count - stream ]] 2 . SELECT 3 . window_start , window_end , error_code , COUNT ( * ) AS error_count 4 . FROM 5 . TABLE ( 6 . TUMBLE ( 7 . TABLE [[ STREAM : iot - monitoring - stream ]], 8 . DESCRIPTOR ( ts ), 9 . INTERVAL '1' MINUTE 10 . ) 11 . ) 12 . WHERE type = 'ERROR' 12 . GROUP BY 13 . window_start , window_end , error_code Note that the input stream does not provide a time field for the time when the authorization attempt was performed. Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional event_time field to the authorization attempts data schema. The above streaming query will produce a continuous stream ( error-count-stream ) events like: { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C007_INVALID_ARGUMENT\" , \"error_count\" : 16 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C011_RESOURCE_EXHAUSTED\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C008_NOT_FOUND\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:19:00\" , \"window_end\" : \"2022-02-16 14:20:00\" , \"error_code\" : \"C001_ABORTED\" , \"error_count\" : 26 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C012_UNAVAILABLE\" , \"error_count\" : 32 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C007_INVALID_ARGUMENT\" , \"error_count\" : 31 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C006_INTERNAL\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C002_ALREADY_EXISTS\" , \"error_count\" : 31 } { \"window_start\" : \"2022-02-16 14:21:00\" , \"window_end\" : \"2022-02-16 14:22:00\" , \"error_code\" : \"C005_FAILED_PRECONDITION\" , \"error_count\" : 24 } { \"window_start\" : \"2022-02-16 14:21:00\" , \"window_end\" : \"2022-02-16 14:22:00\" , \"error_code\" : \"C002_ALREADY_EXISTS\" , \"error_count\" : 20 } { \"window_start\" : \"2022-02-16 14:21:00\" , \"window_end\" : \"2022-02-16 14:22:00\" , \"error_code\" : \"C009_OUT_OF_RANGE\" , \"error_count\" : 20 } Next we can register a User Defined Function (UDF) to process each newly computed error-count-stream events. The UDF function can be implemented in any programming language as long as they adhere to the Streaming-Runtime gRPC protocol. Our UDFs for example, can look for the root causes of the frequently occurring error or send alerting notifications to 3rd party systems. Following diagram illustrates the implementation flow and involved resources: Quick start Follow the Streaming Runtime Install instructions to instal the operator.``` Install the IoT monitoring streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/streaming-pipeline.yaml' -n streaming-runtime Install the IoT monitoring random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/data-generator.yaml' -n default Follow the explore Kafka and explore Rabbit to see what data is generated and how it is processed though the pipeline. Delete the Top-k songs pipeline and the demo song generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = iot-monitoring-data-generator Implementation details One possible way of implementing the above scenario with the help of the Streaming Runtime is to define three Streams and one Processor custom resources and use the Processor's built-in query capabilities. (Note: for the purpose of the demo we will skip the explicit CusterStream definitions and instead will enable auth-provisioning for those). Given that the input iot-monitoring stream uses an Avro data format like this: namespace : com.tanzu.streaming.runtime.iot.log type : record name : MonitoringStream fields : - name : error_code type : string - name : ts type : type : long logicalType : timestamp-millis - name : type type : string - name : application type : string - name : version type : string - name : description type : string We can represent it with the following custom Stream resource: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : iot-monitoring-stream spec : protocol : \"kafka\" storage : clusterStream : \"iot-monitoring-cluster-stream\" streamMode : [ \"read\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : MonitoringStream fields : - name : error_code type : string - name : type type : string - name : application type : string - name : version type : string - name : description type : string - name : ts type : long_timestamp-millis watermark : \"`ts` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The ts field is the timestamp when the event was emitted. We are adding also a 3 seconds watermark to tolerate out-of-order or late coming events! Then the new error-count-stream populated from the fraud detection processor (using JSON format): apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : error-count-stream spec : protocol : \"kafka\" storage : clusterStream : \"error-count-cluster-stream\" streamMode : [ \"read\" , \"write\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : ErrorCount fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : error_code type : string - name : error_count type : long options : ddl.key.fields : error_code ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset The streaming Processor can be defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : possible-fraud-processor spec : inputs : query : - \"INSERT INTO [[STREAM:error-count-stream]] SELECT window_start, window_end, error_code, COUNT(*) AS error_count FROM TABLE(TUMBLE(TABLE [[STREAM:iot-monitoring-stream]], DESCRIPTOR(ts), INTERVAL '1' MINUTE)) WHERE type='ERROR' GROUP BY window_start, window_end, error_code\" sources : - name : \"error-count-stream\" outputs : - name : \"udf-output-error-count-stream\" template : spec : containers : - name : iot-monitoring-error-code-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the udf-output-error-count-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-error-count-stream spec : keys : [ \"error_code\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"udf-output-error-count-cluster-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"IoT Monitoring"},{"location":"samples/iot-monitoring/iot-monitoring/#real-time-iot-log-monitoring","text":"Imagine an IoT network, such as network of sensors, emitting monitoring events into a central service, into a topic iot-monitoring-stream . Such a monitoring stream may look something like this: { \"error_code\" : \"C009_OUT_OF_RANGE\" , \"ts\" : 1645020042399 , \"type\" : \"ERROR\" , \"application\" : \"Hatity\" , \"version\" : \"1.16.4 \" , \"description\" : \"Chuck Norris can binary search unsorted data.\" } { \"error_code\" : \"C014_UNKNOWN\" , \"ts\" : 1645020042400 , \"type\" : \"DEBUG\" , \"application\" : \"Mat Lam Tam\" , \"version\" : \"5.0.9 \" , \"description\" : \"Chuck Norris doesn't bug hunt, as that signifies a probability of failure. He goes bug killing.\" } { \"error_code\" : \"C005_FAILED_PRECONDITION\" , \"ts\" : 1645020042400 , \"type\" : \"ERROR\" , \"application\" : \"Regrant\" , \"version\" : \"2.4.2 \" , \"description\" : \"There is no Esc key on Chuck Norris' keyboard, because no one escapes Chuck Norris.\" } { \"error_code\" : \"C012_UNAVAILABLE\" , \"ts\" : 1645020042401 , \"type\" : \"INFO\" , \"application\" : \"Zontrax\" , \"version\" : \"6.17.2 \" , \"description\" : \"Chuck Norris does not use exceptions when programming. He has not been able to identify any of his code that is not exceptional.\" } { \"error_code\" : \"C006_INTERNAL\" , \"ts\" : 1645020042402 , \"type\" : \"WARN\" , \"application\" : \"Vagram\" , \"version\" : \"5.9.3 \" , \"description\" : \"Quantum cryptography does not work on Chuck Norris. When something is being observed by Chuck it stays in the same state until he's finished.\" } { \"error_code\" : \"C011_RESOURCE_EXHAUSTED\" , \"ts\" : 1645020042402 , \"type\" : \"INFO\" , \"application\" : \"Cardguard\" , \"version\" : \"7.12.19 \" , \"description\" : \"All browsers support the hex definitions #chuck and #norris for the colors black and blue.\" } { \"error_code\" : \"C012_UNAVAILABLE\" , \"ts\" : 1645020042402 , \"type\" : \"ERROR\" , \"application\" : \"Zaam-Dox\" , \"version\" : \"3.12.18 \" , \"description\" : \"Chuck Norris can use GOTO as much as he wants to. Telling him otherwise is considered harmful.\" } { \"error_code\" : \"C013_UNIMPLEMENTED\" , \"ts\" : 1645020042403 , \"type\" : \"INFO\" , \"application\" : \"Aerified\" , \"version\" : \"3.16.11-SNAPSHOT\" , \"description\" : \"Chuck Norris's first program was kill -9.\" } ... The ts field contains the time (e.g. timestamp) when the monitoring event was emitted. From the monitoring stream we will do some filtering ( 12 ), because we only want the monitoring events the represent error events. Then we will group by error_code ( 12-13 ) so that the newly computed, output stream is going to have just the error events and a count of how often each error event occur. The last we will perform time windowing aggregation ( 6-10 ) so that this output would be how many times each error event has occurred in the most recent 1 minute . We can express such analysis with a streaming SQL query like this: 1 . INSERT INTO [[ STREAM : error - count - stream ]] 2 . SELECT 3 . window_start , window_end , error_code , COUNT ( * ) AS error_count 4 . FROM 5 . TABLE ( 6 . TUMBLE ( 7 . TABLE [[ STREAM : iot - monitoring - stream ]], 8 . DESCRIPTOR ( ts ), 9 . INTERVAL '1' MINUTE 10 . ) 11 . ) 12 . WHERE type = 'ERROR' 12 . GROUP BY 13 . window_start , window_end , error_code Note that the input stream does not provide a time field for the time when the authorization attempt was performed. Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional event_time field to the authorization attempts data schema. The above streaming query will produce a continuous stream ( error-count-stream ) events like: { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C007_INVALID_ARGUMENT\" , \"error_count\" : 16 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C011_RESOURCE_EXHAUSTED\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:18:00\" , \"window_end\" : \"2022-02-16 14:19:00\" , \"error_code\" : \"C008_NOT_FOUND\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:19:00\" , \"window_end\" : \"2022-02-16 14:20:00\" , \"error_code\" : \"C001_ABORTED\" , \"error_count\" : 26 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C012_UNAVAILABLE\" , \"error_count\" : 32 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C007_INVALID_ARGUMENT\" , \"error_count\" : 31 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C006_INTERNAL\" , \"error_count\" : 28 } { \"window_start\" : \"2022-02-16 14:20:00\" , \"window_end\" : \"2022-02-16 14:21:00\" , \"error_code\" : \"C002_ALREADY_EXISTS\" , \"error_count\" : 31 } { \"window_start\" : \"2022-02-16 14:21:00\" , \"window_end\" : \"2022-02-16 14:22:00\" , \"error_code\" : \"C005_FAILED_PRECONDITION\" , \"error_count\" : 24 } { \"window_start\" : \"2022-02-16 14:21:00\" , \"window_end\" : \"2022-02-16 14:22:00\" , \"error_code\" : \"C002_ALREADY_EXISTS\" , \"error_count\" : 20 } { \"window_start\" : \"2022-02-16 14:21:00\" , \"window_end\" : \"2022-02-16 14:22:00\" , \"error_code\" : \"C009_OUT_OF_RANGE\" , \"error_count\" : 20 } Next we can register a User Defined Function (UDF) to process each newly computed error-count-stream events. The UDF function can be implemented in any programming language as long as they adhere to the Streaming-Runtime gRPC protocol. Our UDFs for example, can look for the root causes of the frequently occurring error or send alerting notifications to 3rd party systems. Following diagram illustrates the implementation flow and involved resources:","title":"Real Time IoT Log Monitoring"},{"location":"samples/iot-monitoring/iot-monitoring/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator.``` Install the IoT monitoring streaming application: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/streaming-pipeline.yaml' -n streaming-runtime Install the IoT monitoring random data stream: kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/data-generator.yaml' -n default Follow the explore Kafka and explore Rabbit to see what data is generated and how it is processed though the pipeline. Delete the Top-k songs pipeline and the demo song generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = iot-monitoring-data-generator","title":"Quick start"},{"location":"samples/iot-monitoring/iot-monitoring/#implementation-details","text":"One possible way of implementing the above scenario with the help of the Streaming Runtime is to define three Streams and one Processor custom resources and use the Processor's built-in query capabilities. (Note: for the purpose of the demo we will skip the explicit CusterStream definitions and instead will enable auth-provisioning for those). Given that the input iot-monitoring stream uses an Avro data format like this: namespace : com.tanzu.streaming.runtime.iot.log type : record name : MonitoringStream fields : - name : error_code type : string - name : ts type : type : long logicalType : timestamp-millis - name : type type : string - name : application type : string - name : version type : string - name : description type : string We can represent it with the following custom Stream resource: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : iot-monitoring-stream spec : protocol : \"kafka\" storage : clusterStream : \"iot-monitoring-cluster-stream\" streamMode : [ \"read\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : MonitoringStream fields : - name : error_code type : string - name : type type : string - name : application type : string - name : version type : string - name : description type : string - name : ts type : long_timestamp-millis watermark : \"`ts` - INTERVAL '3' SECONDS\" options : ddl.scan.startup.mode : earliest-offset The ts field is the timestamp when the event was emitted. We are adding also a 3 seconds watermark to tolerate out-of-order or late coming events! Then the new error-count-stream populated from the fraud detection processor (using JSON format): apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : error-count-stream spec : protocol : \"kafka\" storage : clusterStream : \"error-count-cluster-stream\" streamMode : [ \"read\" , \"write\" ] keys : [ \"error_code\" ] dataSchemaContext : schema : namespace : com.tanzu.streaming.runtime.iot.log name : ErrorCount fields : - name : window_start type : long_timestamp-millis - name : window_end type : long_timestamp-millis - name : error_code type : string - name : error_count type : long options : ddl.key.fields : error_code ddl.value.format : \"json\" ddl.properties.allow.auto.create.topics : \"true\" ddl.scan.startup.mode : earliest-offset The streaming Processor can be defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Processor metadata : name : possible-fraud-processor spec : inputs : query : - \"INSERT INTO [[STREAM:error-count-stream]] SELECT window_start, window_end, error_code, COUNT(*) AS error_count FROM TABLE(TUMBLE(TABLE [[STREAM:iot-monitoring-stream]], DESCRIPTOR(ts), INTERVAL '1' MINUTE)) WHERE type='ERROR' GROUP BY window_start, window_end, error_code\" sources : - name : \"error-count-stream\" outputs : - name : \"udf-output-error-count-stream\" template : spec : containers : - name : iot-monitoring-error-code-udf image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1 Note that the UDF function can be implemented in any programming language. Finally, the output of the UDF function is send to the udf-output-error-count-stream stream defined like this: apiVersion : streaming.tanzu.vmware.com/v1alpha1 kind : Stream metadata : name : udf-output-error-count-stream spec : keys : [ \"error_code\" ] streamMode : [ \"write\" ] protocol : \"rabbitmq\" storage : clusterStream : \"udf-output-error-count-cluster-stream\" It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.","title":"Implementation details"},{"location":"samples/top-k-songs/top-k-songs/","text":"Top-K Songs By Genre Music ranking application that continuously computes the latest Top 3 music charts based on song play events collected in real-time. This application is modelled as a streaming music service with two input streams: kafka-stream-songs and kafka-stream-playevents and one output stream rabbitmq-stream-1 . The kafka-stream-songs contains all the songs available in the streaming service. The kafka-stream-playevents on the other hand is a feed of songs being played by streaming service. The output rabbitmq-stream-1 stream contains the top-3 songs per genre for the last minute. The output content if capitalized with the help of a user defined function (UDF). First we enrich the kafka-stream-playevents stream by joining it with the kafka-stream-playevents input. The result joined stream, kafka-stream-songplays , contain information for the songs being played as well as the details for those songs, such as name and genre. 1 . INSERT INTO [[ STREAM : kafka - stream - songplays ]] 2 . SELECT 3 . Plays . song_id , Songs . album , Songs . artist , Songs . name , Songs . genre , Plays . duration , Plays . event_time 4 . FROM ( 5 . SELECT * FROM [[ STREAM : kafka - stream - playevents ]] WHERE duration >= 30000 6 . ) AS Plays 7 . INNER JOIN 8 . [[ STREAM : kafka - stream - songs ]] AS Songs ON Plays . song_id = Songs . song_id Additionally, we filter the play events to only accept events where the duration is > 30 seconds (the duration field is in [ms]). Next, we group the kafka-stream-songplays by name and genre over a time-windowed interval and continuously compute the top 3 songs per genre over this interval. 1 . INSERT INTO [[ STREAM : kafka - stream - topk - songs - per - genre ]] 2 . SELECT window_start , window_end , song_id , name , genre , play_count 3 . FROM ( 4 . SELECT * , ROW_NUMBER () OVER ( PARTITION BY window_start , window_end , genre ORDER BY play_count DESC ) AS row_num 5 . FROM ( 6 . SELECT window_start , window_end , song_id , name , genre , COUNT ( * ) AS play_count 7 . FROM TABLE ( TUMBLE ( TABLE [[ STREAM : kafka - stream - songplays ]], DESCRIPTOR ( event_time ), INTERVAL '60' SECONDS )) 8 . GROUP BY window_start , window_end , song_id , name , genre 9 . ) 10 . ) WHERE row_num <= 3 Finally, the aggregated kafka-stream-topk-songs-per-genre stream is passed to a user defined function where, the payload is programmatically altered and the result is output downstream to the rabbitmq-stream-1 stream. class MessageService ( MessageService_pb2_grpc . MessagingServiceServicer ): def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) The top-k-songs.yaml implements the above pipeline using the Streaming-Runtime custom resources: ClusterStream , Stream and Processor . The use case is inspired by the music Kafka sample. Quick start Follow the Streaming Runtime Install instructions to instal the operator. Deploy the Top-K pipeline. Three alternative deployment configurations are provided to demonstrate different approaches to define the payload schemas. streaming-pipeline.yaml with SQL schema with Avro schema kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-sample/top-k-songs/streaming-pipeline-inline-sql-schema.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-sample/top-k-songs/streaming-pipeline-inline-avro-schema.yaml' -n streaming-runtime Start the Songs and PlayEvents message generator. Messages are encoded in Avro, using the same schemas defined by the kafka-stream-songs and kafka-stream-playevents Streams and send to the topics defined in those streams. kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-sample/top-k-songs/data-generator.yaml' -n default Check the topics Use the kubectl get all to find the Kafka broker pod name and then kubectl exec -it pod/<your-kafka-pod> -- /bin/bash ` to SSH to kafka broker container. From within the kafka-broker container use the bin utils to list the topics or check their content: /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092 /opt/kafka/bin/kafka-console-consumer.sh --topic kafka-stream-songs --from-beginning --bootstrap-server localhost:9092 /opt/kafka/bin/kafka-console-consumer.sh --topic kafka-stream-playevents --from-beginning --bootstrap-server localhost:9092 /opt/kafka/bin/kafka-console-consumer.sh --topic kafka-stream-songplays --from-beginning --bootstrap-server localhost:9092 kubectl port-forward svc/rabbitmq 15672 :15672 Open http://localhost:15672/#/exchanges and you should see the dataOut amongst the list. Open the Queues tab and create new queue called pipelineOut (use the default configuration). Open the Exchang tab, select the dataOut exchange and bind it to the pipelineOut queue. Use the # as a Routing key . From the Queue tab select the pipelineOut queue and click the Get Messages button. In addition, you can check the streaming-runtime-processor pod for logs like: +----+---------------+------------+---------+------------------+---------+------------+ | op | window_start | window_end | song_id | name | genre | play_count | +----+---------------+------------+---------+------------------+---------+------------+ | +I | 2022 -01-19 .. | ... | 2 | Animal | Punk | 21 | | +I | 2022 -01-19 .. | ... | 1 | Chemical Warfare | Punk | 19 | | +I | 2022 -01-19 .. | ... | 5 | Punks Not Dead | Punk | 16 | | +I | 2022 -01-19 .. | ... | 11 | Fack | Hip Hop | 18 | | +I | 2022 -01-19 .. | ... | 10 | 911 Is A Joke | Hip Hop | 15 | | +I | 2022 -01-19 .. | ... | 12 | The Calling | Hip Hop | 15 | ... (Note: this logs are result of the processor's debug.query: SELECT * FROM TopKSongsPerGenre ). Delete the Top-k songs pipeline and the demo song generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = top-k-songs-data-generator #to stop the legacy generator kubectl delete deployments -l app = songs-generator","title":"Music Chart"},{"location":"samples/top-k-songs/top-k-songs/#top-k-songs-by-genre","text":"Music ranking application that continuously computes the latest Top 3 music charts based on song play events collected in real-time. This application is modelled as a streaming music service with two input streams: kafka-stream-songs and kafka-stream-playevents and one output stream rabbitmq-stream-1 . The kafka-stream-songs contains all the songs available in the streaming service. The kafka-stream-playevents on the other hand is a feed of songs being played by streaming service. The output rabbitmq-stream-1 stream contains the top-3 songs per genre for the last minute. The output content if capitalized with the help of a user defined function (UDF). First we enrich the kafka-stream-playevents stream by joining it with the kafka-stream-playevents input. The result joined stream, kafka-stream-songplays , contain information for the songs being played as well as the details for those songs, such as name and genre. 1 . INSERT INTO [[ STREAM : kafka - stream - songplays ]] 2 . SELECT 3 . Plays . song_id , Songs . album , Songs . artist , Songs . name , Songs . genre , Plays . duration , Plays . event_time 4 . FROM ( 5 . SELECT * FROM [[ STREAM : kafka - stream - playevents ]] WHERE duration >= 30000 6 . ) AS Plays 7 . INNER JOIN 8 . [[ STREAM : kafka - stream - songs ]] AS Songs ON Plays . song_id = Songs . song_id Additionally, we filter the play events to only accept events where the duration is > 30 seconds (the duration field is in [ms]). Next, we group the kafka-stream-songplays by name and genre over a time-windowed interval and continuously compute the top 3 songs per genre over this interval. 1 . INSERT INTO [[ STREAM : kafka - stream - topk - songs - per - genre ]] 2 . SELECT window_start , window_end , song_id , name , genre , play_count 3 . FROM ( 4 . SELECT * , ROW_NUMBER () OVER ( PARTITION BY window_start , window_end , genre ORDER BY play_count DESC ) AS row_num 5 . FROM ( 6 . SELECT window_start , window_end , song_id , name , genre , COUNT ( * ) AS play_count 7 . FROM TABLE ( TUMBLE ( TABLE [[ STREAM : kafka - stream - songplays ]], DESCRIPTOR ( event_time ), INTERVAL '60' SECONDS )) 8 . GROUP BY window_start , window_end , song_id , name , genre 9 . ) 10 . ) WHERE row_num <= 3 Finally, the aggregated kafka-stream-topk-songs-per-genre stream is passed to a user defined function where, the payload is programmatically altered and the result is output downstream to the rabbitmq-stream-1 stream. class MessageService ( MessageService_pb2_grpc . MessagingServiceServicer ): def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) The top-k-songs.yaml implements the above pipeline using the Streaming-Runtime custom resources: ClusterStream , Stream and Processor . The use case is inspired by the music Kafka sample.","title":"Top-K Songs By Genre"},{"location":"samples/top-k-songs/top-k-songs/#quick-start","text":"Follow the Streaming Runtime Install instructions to instal the operator. Deploy the Top-K pipeline. Three alternative deployment configurations are provided to demonstrate different approaches to define the payload schemas. streaming-pipeline.yaml with SQL schema with Avro schema kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-sample/top-k-songs/streaming-pipeline-inline-sql-schema.yaml' -n streaming-runtime kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-sample/top-k-songs/streaming-pipeline-inline-avro-schema.yaml' -n streaming-runtime Start the Songs and PlayEvents message generator. Messages are encoded in Avro, using the same schemas defined by the kafka-stream-songs and kafka-stream-playevents Streams and send to the topics defined in those streams. kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-sample/top-k-songs/data-generator.yaml' -n default Check the topics Use the kubectl get all to find the Kafka broker pod name and then kubectl exec -it pod/<your-kafka-pod> -- /bin/bash ` to SSH to kafka broker container. From within the kafka-broker container use the bin utils to list the topics or check their content: /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092 /opt/kafka/bin/kafka-console-consumer.sh --topic kafka-stream-songs --from-beginning --bootstrap-server localhost:9092 /opt/kafka/bin/kafka-console-consumer.sh --topic kafka-stream-playevents --from-beginning --bootstrap-server localhost:9092 /opt/kafka/bin/kafka-console-consumer.sh --topic kafka-stream-songplays --from-beginning --bootstrap-server localhost:9092 kubectl port-forward svc/rabbitmq 15672 :15672 Open http://localhost:15672/#/exchanges and you should see the dataOut amongst the list. Open the Queues tab and create new queue called pipelineOut (use the default configuration). Open the Exchang tab, select the dataOut exchange and bind it to the pipelineOut queue. Use the # as a Routing key . From the Queue tab select the pipelineOut queue and click the Get Messages button. In addition, you can check the streaming-runtime-processor pod for logs like: +----+---------------+------------+---------+------------------+---------+------------+ | op | window_start | window_end | song_id | name | genre | play_count | +----+---------------+------------+---------+------------------+---------+------------+ | +I | 2022 -01-19 .. | ... | 2 | Animal | Punk | 21 | | +I | 2022 -01-19 .. | ... | 1 | Chemical Warfare | Punk | 19 | | +I | 2022 -01-19 .. | ... | 5 | Punks Not Dead | Punk | 16 | | +I | 2022 -01-19 .. | ... | 11 | Fack | Hip Hop | 18 | | +I | 2022 -01-19 .. | ... | 10 | 911 Is A Joke | Hip Hop | 15 | | +I | 2022 -01-19 .. | ... | 12 | The Calling | Hip Hop | 15 | ... (Note: this logs are result of the processor's debug.query: SELECT * FROM TopKSongsPerGenre ). Delete the Top-k songs pipeline and the demo song generator: kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments -l app = top-k-songs-data-generator #to stop the legacy generator kubectl delete deployments -l app = songs-generator","title":"Quick start"},{"location":"udf/architecture/","text":"The Streaming Runtime provides a GrpcMessage schema to model the messages exchanged between the Multibinder and the User Defined Functions (UDF). It also defines a MessagingService offering four interaction modes to choose from. syntax = \"proto3\" ; message GrpcMessage { bytes payload = 1 ; map < string , string > headers = 2 ; } service MessagingService { rpc biStream ( stream GrpcMessage ) returns ( stream GrpcMessage ); rpc clientStream ( stream GrpcMessage ) returns ( GrpcMessage ); rpc serverStream ( GrpcMessage ) returns ( stream GrpcMessage ); rpc requestReply ( GrpcMessage ) returns ( GrpcMessage ); } The MessageService.proto allows you to generate required stubs to support true polyglot nature of gRPC while interacting with functions hosted by Streaming Runtime . The Multibinder forwards the incoming messages over the MessagingService to the pre-configured UDF function. The UDF response in turn is sent to the Multibinder's output stream. If the Time Windowing Aggregation is enabled, the multibinder will collect all messages part of the window and pass them at once to the UDF to compute aggregated state. Interaction RPC Modes The MessagingService gRPC provides 4 interaction modes: Reques/Repply RPC Server-side streaming RPC Client-side streaming RPC Bi-directional streaming RPC Request Reply RPC The most straight forward interaction mode is Request/Reply. Suppose you have a function in Java: public Function < String , String > uppercase () { return v -> v . toUpperCase (); } Python: def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) Or GoLang: func ( s * server ) RequestReply ( ctx context . Context , in * pb . GrpcMessage ) ( * pb . GrpcMessage , error ) { log . Printf ( \"Received: %v\" , string ( in . Payload )) upperCasePayload := strings . ToUpper ( string ( in . Payload )) return & pb . GrpcMessage { Payload : [] byte ( upperCasePayload )}, nil } Server-side streaming RPC WIP Client-side streaming RPC WIP Bi-Directional streaming RPC If you are building your UDF in Java you can find more information about the Spring Cloud Function gRPC support here .","title":"Architecture"},{"location":"udf/architecture/#interaction-rpc-modes","text":"The MessagingService gRPC provides 4 interaction modes: Reques/Repply RPC Server-side streaming RPC Client-side streaming RPC Bi-directional streaming RPC","title":"Interaction RPC Modes"},{"location":"udf/architecture/#request-reply-rpc","text":"The most straight forward interaction mode is Request/Reply. Suppose you have a function in Java: public Function < String , String > uppercase () { return v -> v . toUpperCase (); } Python: def requestReply ( self , request , context ): print ( \"Server received Payload: %s and Headers: %s \" % ( request . payload . decode (), request . headers )) return MessageService_pb2 . GrpcMessage ( payload = str . encode ( request . payload . decode () . upper ()), headers = request . headers ) Or GoLang: func ( s * server ) RequestReply ( ctx context . Context , in * pb . GrpcMessage ) ( * pb . GrpcMessage , error ) { log . Printf ( \"Received: %v\" , string ( in . Payload )) upperCasePayload := strings . ToUpper ( string ( in . Payload )) return & pb . GrpcMessage { Payload : [] byte ( upperCasePayload )}, nil }","title":"Request Reply RPC"},{"location":"udf/architecture/#server-side-streaming-rpc","text":"WIP","title":"Server-side streaming RPC"},{"location":"udf/architecture/#client-side-streaming-rpc","text":"WIP","title":"Client-side streaming RPC"},{"location":"udf/architecture/#bi-directional-streaming-rpc","text":"If you are building your UDF in Java you can find more information about the Spring Cloud Function gRPC support here .","title":"Bi-Directional streaming RPC"},{"location":"udf/usage/","text":"The udf-uppercase-java , udf-uppercase-go and udf-uppercase-python sample projects show how to build simple UDFs in Java , Python or Go using the Reques/Repply RPC mode. Also, you can find there instructions how to build the UDF container image and push those to the container registry of choice. For example in case of the Python UDF you can use a Dockerfile like this: FROM python:3.9.7-slim RUN pip install grpcio RUN pip install grpcio-tools ADD MessageService_pb2.py / ADD MessageService_pb2_grpc.py / ADD message_service_server.py / ENTRYPOINT [ \"python\" , \"/message_service_server.py\" ] CMD [] to build the container image: docker build -t ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 . and push it to the registry: docker push ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 Then you can refer this image from within your streaming Processor CR definitions. For example: 1. apiVersion : streaming.tanzu.vmware.com/v1alpha1 2. kind : Processor 3. metadata : 4. name : my-streaming-processor 5. spec : 6. inputs : 7. - name : \"my-input-stream\" # input streams for the UDF function 8. outputs : 9. - name : \"my-output-stream\" # output streams for the UDF function 10. template : 11. spec : 12. containers : 13. - name : my-python-udf-container 14. image : ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 Note that the my-python-udf-container (lines 13 - 14 ) uses the udf-uppercase-python:0.1 image. When deployed by the streaming runtime this processor would look like this:","title":"Usage"}]}